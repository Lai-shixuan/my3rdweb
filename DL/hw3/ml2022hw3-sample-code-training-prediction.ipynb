{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Includes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "_exp_name = \"sample\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:35.597455300Z",
     "start_time": "2023-11-14T01:42:35.553639100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "import pandas as pd\n",
    "\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "papermill": {
     "duration": 1.654263,
     "end_time": "2022-02-23T10:03:07.947242",
     "exception": false,
     "start_time": "2022-02-23T10:03:06.292979",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-04T07:49:47.240504Z",
     "iopub.execute_input": "2022-03-04T07:49:47.240907Z",
     "iopub.status.idle": "2022-03-04T07:49:48.860296Z",
     "shell.execute_reply.started": "2022-03-04T07:49:47.240872Z",
     "shell.execute_reply": "2022-03-04T07:49:48.859536Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:35.660052200Z",
     "start_time": "2023-11-14T01:42:35.587428100Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "myseed = 6666  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.078771,
     "end_time": "2022-02-23T10:03:08.039428",
     "exception": false,
     "start_time": "2022-02-23T10:03:07.960657",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-04T07:49:48.862365Z",
     "iopub.execute_input": "2022-03-04T07:49:48.862631Z",
     "iopub.status.idle": "2022-03-04T07:49:48.914697Z",
     "shell.execute_reply.started": "2022-03-04T07:49:48.862595Z",
     "shell.execute_reply": "2022-03-04T07:49:48.913996Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:35.673599400Z",
     "start_time": "2023-11-14T01:42:35.665591100Z"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transforms\n",
    "Torchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n",
    "\n",
    "Please refer to PyTorch official website for details about different transforms."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.01289,
     "end_time": "2022-02-23T10:03:08.065357",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.052467",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Normally, We don't need augmentations in testing and validation.\n",
    "# All we need here is to resize the PIL image and transform it into Tensor.\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# However, it is also possible to use augmentation in the testing phase.\n",
    "# You may use train_tfm to produce a variety of images and then test using ensemble methods\n",
    "train_tfm = transforms.Compose([\n",
    "    # Resize the image into a fixed shape (height = width = 128)\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomGrayscale(),\n",
    "    # transforms.RandomResizedCrop,\n",
    "    # transforms.AutoAugment(),\n",
    "    # You may add some transforms here.\n",
    "    # ToTensor() should be the last one of the transforms.\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.021406,
     "end_time": "2022-02-23T10:03:08.099437",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.078031",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-04T07:49:48.918011Z",
     "iopub.execute_input": "2022-03-04T07:49:48.919438Z",
     "iopub.status.idle": "2022-03-04T07:49:48.924946Z",
     "shell.execute_reply.started": "2022-03-04T07:49:48.919373Z",
     "shell.execute_reply": "2022-03-04T07:49:48.924086Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:35.685623Z",
     "start_time": "2023-11-14T01:42:35.675745200Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets\n",
    "The data is labelled by the name, so we load images and label while calling '__getitem__'"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.012739,
     "end_time": "2022-02-23T10:03:08.125181",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.112442",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FoodDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tfm=test_tfm, files = None):\n",
    "        super(FoodDataset).__init__()\n",
    "        # self.path = path\n",
    "        # self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        self.files = files \n",
    "        # if files != None:\n",
    "        #     self.files = files\n",
    "        # print(f\"One sample\",self.files[0])\n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "        #im = self.data[idx]\n",
    "        try:\n",
    "            label = int(os.path.split(fname)[1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "        return im,label"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.023022,
     "end_time": "2022-02-23T10:03:08.160912",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.13789",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-04T07:49:48.927868Z",
     "iopub.execute_input": "2022-03-04T07:49:48.928286Z",
     "iopub.status.idle": "2022-03-04T07:49:48.93946Z",
     "shell.execute_reply.started": "2022-03-04T07:49:48.928256Z",
     "shell.execute_reply": "2022-03-04T07:49:48.938642Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:35.716075400Z",
     "start_time": "2023-11-14T01:42:35.688631700Z"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "_dataset_dir = \"./data/\"\n",
    "\n",
    "def dataset_prefold(path):\n",
    "    dataset = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "    return dataset\n",
    "\n",
    "train_set = dataset_prefold(os.path.join(_dataset_dir,\"training\"))\n",
    "val_set = dataset_prefold(os.path.join(_dataset_dir,\"validation\"))\n",
    "train_val_set = train_set + val_set\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_val_encoded = le.fit_transform(train_val_set)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:35.895126800Z",
     "start_time": "2023-11-14T01:42:35.695481500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# The number of training epochs and patience.\n",
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "patience = 300 # If no improvement in 'patience' epochs, early stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:35.896136700Z",
     "start_time": "2023-11-14T01:42:35.738836700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        # input 維度 [3, 128, 128]\n",
    "        \n",
    "        #create a deep learning network using Resnet-50 architecture\n",
    "        self.cnn = torch.hub.load('pytorch/vision:v0.10.0','resnet50', pretrained=False)\n",
    "        \n",
    "        # self.cnn = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, 3, 1, 1),  # [64, 128, 128]\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "        # \n",
    "        #     nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 128, 3, 1, 1), # [128, 64, 64]\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "        # \n",
    "        #     nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(256, 256, 3, 1, 1), # [256, 32, 32]\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(256, 256, 3, 1, 1), # [256, 32, 32]\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "        # \n",
    "        #     nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(512, 512, 3, 1, 1), # [512, 16, 16]\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(512, 512, 3, 1, 1), # [512, 16, 16]\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "        #     \n",
    "        #     nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
    "        # )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048*7*7, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)   #out的类型是torch.Tensor，大小是[batch_size, 2048, 7, 7]\n",
    "        print(out.size())\n",
    "        out = out.view(out.size()[0], -1)   #作用是：将out的维度变为[batch_size, 1000]\n",
    "        print(out.size())\n",
    "        return self.fc(out)\n",
    "    \n",
    "    #tensor.view()函数的意思是将tensor的维度变为新的维度。例如，tensor.view(1, -1)就是将tensor的维度变为[1, -1]，即将tensor的第一个维度变为1，其余维度变为-1，即将tensor的维度变为一维。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T02:01:07.377095500Z",
     "start_time": "2023-11-14T02:01:07.374574200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.cnn.layer4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T02:01:07.806673900Z",
     "start_time": "2023-11-14T02:01:07.801321700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and Validate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "acc_scores = []\n",
    "\n",
    "with open(f\"./log/{_exp_name}_log.csv\",\"a\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"fold\", \"mode\", f\"Epoch = {n_epochs}\", \"Loss\", \"Acc\", \"Note\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T02:01:09.449581Z",
     "start_time": "2023-11-14T02:01:09.437248300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\laish/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\laish\\miniconda3\\envs\\dpln\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\laish\\miniconda3\\envs\\dpln\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "One sample of train ./data/training\\0_1.jpg\n",
      "One sample of val ./data/training\\0_0.jpg\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/167 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68310fa55bc646e49639deecbb8d5f79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n",
      "torch.Size([64, 1000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x1000 and 100352x1000)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 48\u001B[0m\n\u001B[0;32m     43\u001B[0m imgs, labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m#imgs = imgs.half()\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m#print(imgs.shape,labels.shape)\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# Forward the data. (Make sure data and model are on the same device.)\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# Calculate the cross-entropy loss.\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# We don't need to apply softmax before computing cross-entropy as it is done automatically.\u001B[39;00m\n\u001B[0;32m     52\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(logits, labels\u001B[38;5;241m.\u001B[39mto(device))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dpln\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[38], line 72\u001B[0m, in \u001B[0;36mClassifier.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     70\u001B[0m out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mview(out\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)   \u001B[38;5;66;03m#作用是：将out的维度变为[batch_size, 1000]\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(out\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m---> 72\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dpln\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dpln\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dpln\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dpln\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (64x1000 and 100352x1000)"
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_val_set)):\n",
    "    # Initialize a model, and put it on the device specified.\n",
    "    model = Classifier().to(device)\n",
    "\n",
    "    # For the classification task, we use cross-entropy as the measurement of performance.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "    \n",
    "    train_images = [train_val_set[i] for i in train_ids]\n",
    "    val_images = [train_val_set[i] for i in val_ids]\n",
    "    \n",
    "    print(f'FOLD {fold}')\n",
    "    print(f\"One sample of train\",train_images[0])\n",
    "    print(f\"One sample of val\",val_images[0])\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Initialize trackers, these are not parameters and should not be changed\n",
    "    stale = 0\n",
    "    best_acc = 0\n",
    "    \n",
    "    # Construct datasets.\n",
    "    # The argument \"loader\" tells how torchvision reads the data.\n",
    "    train_set = FoodDataset(files=train_images, tfm=train_tfm)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    valid_set = FoodDataset(files=val_images, tfm=test_tfm)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # ---------- Training ----------\n",
    "        # Make sure the model is in train mode before training.\n",
    "        model.train()\n",
    "\n",
    "        # These are used to record information in training.\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "\n",
    "            # A batch consists of image data and corresponding labels.\n",
    "            imgs, labels = batch\n",
    "            #imgs = imgs.half()\n",
    "            #print(imgs.shape,labels.shape)\n",
    "\n",
    "            # Forward the data. (Make sure data and model are on the same device.)\n",
    "            logits = model(imgs.to(device))\n",
    "\n",
    "            # Calculate the cross-entropy loss.\n",
    "            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "\n",
    "            # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the gradients for parameters.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the gradient norms for stable training.\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "            # Update the parameters with computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the accuracy for current batch.\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "            # Record the loss and accuracy.\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "            \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "        # Print the information.\n",
    "        print(f\"{fold}, [ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "        \n",
    "        # Update the log.\n",
    "        with open(f\"./log/{_exp_name}_log.csv\",\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([fold, \"train\", f\"{epoch+1:03d}\", f\"{train_loss:.5f}\", f\"{train_acc:.5f}\", \"\"])\n",
    "\n",
    "        # ---------- Validation ----------\n",
    "        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "        model.eval()\n",
    "\n",
    "        # These are used to record information in validation.\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "\n",
    "        # Iterate the validation set by batches.\n",
    "        for batch in tqdm(valid_loader):\n",
    "\n",
    "            # A batch consists of image data and corresponding labels.\n",
    "            imgs, labels = batch\n",
    "            #imgs = imgs.half()\n",
    "\n",
    "            # We don't need gradient in validation.\n",
    "            # Using torch.no_grad() accelerates the forward process.\n",
    "            with torch.no_grad():\n",
    "                logits = model(imgs.to(device))\n",
    "\n",
    "            # We can still compute the loss (but not the gradient).\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "\n",
    "            # Compute the accuracy for current batch.\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "            # Record the loss and accuracy.\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "            #break\n",
    "\n",
    "        # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "        # Print the information.\n",
    "        print(f\"{fold}, [ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "        # update logs\n",
    "        with open(f\"./log/{_exp_name}_log.csv\",\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            if valid_acc > best_acc:\n",
    "                writer.writerow([fold, \"val\", f\"{epoch+1:03d}\", f\"{valid_loss:.5f}\", f\"{valid_acc:.5f}\", \"best\"])\n",
    "            else:\n",
    "                writer.writerow([fold, \"val\", f\"{epoch+1:03d}\", f\"{valid_loss:.5f}\", f\"{valid_acc:.5f}\", \"\"])\n",
    "        \n",
    "        \n",
    "        # save models\n",
    "        if valid_acc > best_acc:\n",
    "            print(f\"Best model found at epoch {epoch + 1}, saving model\")\n",
    "            torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
    "            best_acc = valid_acc\n",
    "            stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale > patience:\n",
    "                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "                break\n",
    "                \n",
    "    acc_scores.append(best_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T02:01:11.767184500Z",
     "start_time": "2023-11-14T02:01:10.350069300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing and generate prediction CSV"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.498773,
     "end_time": "2022-02-23T19:10:20.961802",
     "exception": false,
     "start_time": "2022-02-23T19:10:20.463029",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set = dataset_prefold(os.path.join(_dataset_dir,\"validation\"))\n",
    "test_set = FoodDataset(files=test_set, tfm=test_tfm)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T01:42:37.129622700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_best = Classifier().to(device)\n",
    "model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n",
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for data,_ in test_loader:\n",
    "        test_pred = model_best(data.to(device))\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        prediction += test_label.squeeze().tolist()"
   ],
   "metadata": {
    "papermill": {
     "duration": 49.157727,
     "end_time": "2022-02-23T19:11:10.61523",
     "exception": false,
     "start_time": "2022-02-23T19:10:21.457503",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-04T07:57:48.800079Z",
     "iopub.execute_input": "2022-03-04T07:57:48.800382Z",
     "iopub.status.idle": "2022-03-04T07:58:26.536525Z",
     "shell.execute_reply.started": "2022-03-04T07:57:48.800344Z",
     "shell.execute_reply": "2022-03-04T07:58:26.535783Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-11-14T01:42:37.132138Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#create test csv\n",
    "def pad4(i):\n",
    "    return \"0\"*(4-len(str(i)))+str(i)\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "month = \"%02d\" % current_time.month\n",
    "day = \"%02d\" % current_time.day\n",
    "hour = \"%02d\" % current_time.hour\n",
    "minute = \"%02d\" % current_time.minute\n",
    "time = f\"{month}{day}{hour}{minute}\"\n",
    "# result_acc = str(round(best_acc.item(), 2))\n",
    "result_acc = str(round(0.67827, 2))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\n",
    "df[\"Category\"] = prediction\n",
    "df.to_csv(\"./result/submission_\" + time + \"_\" + result_acc + \".csv\",index = False)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.554276,
     "end_time": "2022-02-23T19:11:11.870035",
     "exception": false,
     "start_time": "2022-02-23T19:11:11.315759",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-04T07:58:26.537894Z",
     "iopub.execute_input": "2022-03-04T07:58:26.53815Z",
     "iopub.status.idle": "2022-03-04T07:58:26.56908Z",
     "shell.execute_reply.started": "2022-03-04T07:58:26.538114Z",
     "shell.execute_reply": "2022-03-04T07:58:26.56829Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "start_time": "2023-11-14T01:42:37.133140200Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.rename(\"./log/sample_log.csv\", \"./log/samplelog_\" + time + \"_\" + result_acc + \".csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T01:42:37.135645500Z",
     "start_time": "2023-11-14T01:42:37.134137Z"
    }
   }
  }
 ]
}
